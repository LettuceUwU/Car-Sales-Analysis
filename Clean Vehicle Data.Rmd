---
title: "(Clean) Vehicle Data"
author: "Glorija"
date: "2024-06-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
data <- read.csv("C:\\Users\\glori\\Desktop\\R studies\\17.06\\clean_vehicle_data.csv")
library(dplyr)
library(stringr)
library(tidyr)
library(caret)
library(glmnet)
library(polycor)
library(DescTools)
```

## Introduction
This is a presentation of my findings while investigating the dataset car_prices

### The dataset has been cleaned and filtered:
* Removed irrelevant columns such as: trim, vin, body, state, interior, body, seller, and sell date;
* Cleared non-sensical data, missing values, and NAs;
* Made unique makes and colors uniform;
* Converted variables into factors & normalised

## Summary & Structure

```{r data, echo = TRUE}
summary(data)
head(data)
str(data)
```

## Correlation & Feature Selection

### Correlation Matrix (char & num)

- Cramer's V for Categorical vs Categorical comparison
- Pearson's for Numerical vs Numerical comparison
- Cramer's V for Categorical vs Numerical comparison

```{r correlation cn}
data <- data %>%
  mutate(
    condition = as.factor(condition),
    make = as.factor(make),
    transmission = as.factor(transmission),
    color = as.factor(color)
  )
cramers_v <- function(x, y) {
  CV <- sqrt(DescTools::CramerV(table(x, y)))
  return(CV)
}
correlation_matrix <- matrix(NA, ncol = ncol(data), nrow = ncol(data))
colnames(correlation_matrix) <- colnames(data)
rownames(correlation_matrix) <- colnames(data)
for (i in 1:ncol(data)) {
  for (j in 1:ncol(data)) {
    if (is.factor(data[[i]]) & is.factor(data[[j]])) {
      correlation_matrix[i, j] <- cramers_v(data[[i]], data[[j]])
    } else if (is.numeric(data[[i]]) & is.numeric(data[[j]])) {
      correlation_matrix[i, j] <- cor(data[[i]], data[[j]], use = "complete.obs")
    } else if (is.factor(data[[i]]) & is.numeric(data[[j]])) {
      correlation_matrix[i, j] <- sqrt(DescTools::CramerV(table(data[[i]], cut(data[[j]], breaks = 5))))
    } else if (is.numeric(data[[i]]) & is.factor(data[[j]])) {
      correlation_matrix[i, j] <- sqrt(DescTools::CramerV(table(data[[j]], cut(data[[i]], breaks = 5))))
    }
  }
}
rounded_correlation_matrix <- round(correlation_matrix, 3)
print(rounded_correlation_matrix)
```

### Correlation Matrix Continued (char & num)

- Some of the correlation coefficient's are below 0.2, so I increased the threshold and filtered out the small correlations

```{r correlation threshold cn}
target_correlation <- abs(correlation_matrix[, "sellingprice"])
selected_features <- names(target_correlation[target_correlation > 0.2])
print(selected_features)
selected_features <- selected_features[selected_features != "sellingprice"]
selected_data <- data %>%
  select(all_of(selected_features), sellingprice)
selected_data <- selected_data %>%
  mutate_if(is.factor, as.numeric)
revised_correlation_matrix <- cor(selected_data)
rounded_revised_correlation_matrix <- round(revised_correlation_matrix, 3)
print(rounded_revised_correlation_matrix)
```

### PCA (char & num)

```{r pca cn}
selected_data <- read.csv("C:\\Users\\glori\\Desktop\\R studies\\17.06\\selected_features_data.csv")
useful_columns <- c("year", "make", "odometer", "mmr", "sellingprice", "condition")
selected_data <- selected_data %>%
  mutate(
    condition = as.factor(condition),
    make = as.factor(make),
  )
cramers_v <- function(x, y) {
  CV <- sqrt(DescTools::CramerV(table(x, y)))
  return(CV)
}
selected_data <- selected_data %>%
  mutate_if(is.factor, as.numeric)
selected_data <- selected_data %>%
  select(-X)
selected_data <- selected_data %>%
  mutate_if(is.integer, as.numeric)
scaled_data <- scale(selected_data)
pca_result <- stats::prcomp(scaled_data, center = TRUE, scale. = TRUE)
pca_scores <- pca_result$x[, 1:6]
pca_scores <- t(pca_scores)
pca_data <- data.frame(
  variable = useful_columns,
  PC1 = pca_scores[, 1],
  PC2 = pca_scores[, 2],
  PC3 = pca_scores[, 3],
  PC4 = pca_scores[, 4],
  PC5 = pca_scores[, 5],
  PC6 = pca_scores[, 6]
)
summary(pca_result)
loadings <- pca_result$rotation
rounded_loadings <- round(loadings, 3)
print(rounded_loadings)
explained_variance <- pca_result$sdev^2 / sum(pca_result$sdev^2)
variance_percentage <- explained_variance * 100
scree_data <- data.frame(
  Principal_Component = paste0("PC", 1:length(variance_percentage)),
  Variance_Explained = variance_percentage
)
print(scree_data)
```

### Correlation Matrix (num)

- Seeing as there are many PCs and it takes about 4 of them to explain most of the data, I have decided to do correlation with just
numerical variables (low correlation) and see if PCA improves

```{r correlation n}
num_data <- read.csv("C:\\Users\\glori\\Desktop\\R studies\\17.06\\clean_vehicle_data.csv")
num_columns <- c("year", "odometer", "mmr", "sellingprice")
num_data <- num_data %>%
  select(all_of(num_columns))
num_data <- num_data %>%
  mutate_if(is.factor, as.numeric)
num_scaled_data <- scale(num_data)
num_correlation_matrix <- cor(num_scaled_data)
rounded_num_cor <- round(num_correlation_matrix, 3)
print(rounded_num_cor)
```

### PCA (num)

```{r pca n}
numerical_data <- num_data %>%
  select(year, odometer, mmr, sellingprice)
numerical_scaled_data <- scale(numerical_data)
numerical_pca_result <- stats::prcomp(numerical_scaled_data, center = TRUE, scale. = TRUE)
numerical_pca_scores <- numerical_pca_result$rotation
numerical_pca_data <- as.data.frame(cbind(variable = names(num_data), numerical_pca_scores))
num_loadings <- numerical_pca_result$rotation
summary(numerical_pca_result)
rounded_num_loadings <- round(num_loadings, 3)
print(rounded_num_loadings)
num_explained_variance <- numerical_pca_result$sdev^2 / sum(numerical_pca_result$sdev^2)
num_variance_percentage <- num_explained_variance * 100
num_scree_data <- data.frame(
  Principal_Component = paste0("PC", 1:length(num_variance_percentage)),
  Variance_Explained = num_variance_percentage
)
print(num_scree_data)
```

### Correlation & Feature Selection Conclusion

- Numerical variables explain the data better and the dimensions are reduced therefore,
categorical variables have been filtered out for a better model

## Regression Models

### Linear Regression

```{r linear regression}
predictors <- numerical_scaled_data[, -which(colnames(numerical_scaled_data) == "sellingprice")]
response <- numerical_scaled_data[, "sellingprice"]
lm_num_model_scaled <- lm(response ~ ., data = as.data.frame(predictors))
summary(lm_num_model_scaled)
```

### Linear Regression Model Evaluation & Cross-Validation

```{r linear regression eval and cross}
set.seed(123)
train_index <- createDataPartition(scaled_data[, "sellingprice"], p = 0.8, list = FALSE)
train_data <- numerical_scaled_data[train_index, ]
test_data <- numerical_scaled_data[-train_index, ]

lm_predictions <- predict(lm_num_model_scaled, as.data.frame(test_data))
lm_residuals <- test_data[, "sellingprice"] - lm_predictions

lm_rmse <- sqrt(mean(lm_residuals^2))
lm_r2 <- summary(lm_num_model_scaled)$r.squared
cat("Linear Regression RMSE:", lm_rmse, "\n")
cat("Linear Regression R-squared:", lm_r2, "\n")
```

### Polynomial Regression

```{r poly regression}
poly_model <- lm(sellingprice ~ poly(year, 2) + poly(odometer, 2) + poly(mmr, 2), data = as.data.frame(train_data))
summary(poly_model)

poly_predictions <- predict(poly_model, as.data.frame(test_data))
poly_residuals <- test_data[, "sellingprice"] - poly_predictions

poly_rmse <- sqrt(mean(poly_residuals^2))
poly_r2 <- summary(poly_model)$r.squared
cat("Polynomial Regression RMSE:", poly_rmse, "\n")
cat("Polynomial Regression R-squared:", poly_r2, "\n")

```

### Model Comparison

```{r regression model comparison}
lm_residuals_df <- data.frame(Predicted = lm_predictions, Residuals = lm_residuals)
ggplot(lm_residuals_df, aes(x = Predicted, y = Residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, col = "red") +
  labs(title = "Linear Regression Residuals Plot", x = "Predicted", y = "Residuals") +
  theme_minimal()

poly_residuals_df <- data.frame(Predicted = poly_predictions, Residuals = poly_residuals)
ggplot(poly_residuals_df, aes(x = Predicted, y = Residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, col = "red") +
  labs(title = "Polynomial Regression Residuals Plot", x = "Predicted", y = "Residuals") +
  theme_minimal()

```

## Conlcusion

- Given that the RMSE for the linear regression and the R-squared are very close but slightly better than the polynomial regression, the linear model is likely the better choice. It's simpler, less prone to overfitting, and has better predictive performance based on RMSE
